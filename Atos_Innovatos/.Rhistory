names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
nzv <- nearZeroVar(data_balanced)
length(nzv)
data_balanced[is.na(data_balanced)]<--9999
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
xData <- subset(data_balanced,select=-yVar);
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(xData)[names(xData) == "yVar"] <- yVar
yData <- data_balanced[,yVar];
fit.svm<-svm(x= xData,y= yData)
test_data <- data_test
test_result <- test_data[,yVar]; # Make sure we have the results seaprate - as some functions like it separate
names(test_data)[names(test_data) == yVar] <- "yVar"
test_data <- subset(test_data,select=-yVar);
names(test_data)[names(test_data) == "yVar"] <- yVar
test_data[is.na(test_data)]<--9999 #just some random number that never happened in the data
test_data<-subset(test_data,select = setdiff(columns.to.keep,yVar)) #the columns will stay which has less than 50% NAs
length(nzv)
pred_data <- predict(object = fit.svm, newdata = test_data)
xtab <- table(pred_data, test_result)
data <- data
output$Out_svm = renderPrint({
isolate({
cat("Started with options:\r\n")
cat("* SMOTE : ", input$chkSVMWithSMOTE,"\r\n")
#cat("* TOP100 Features : ",input$chkSVMWithTOPFeatures,"\r\n")
cat("* TEST/TRAINING SET : ",input$chkSVMWithTest,"\r\n")
cat("---------------------------------------------------------\r\n")
confusionMatrix(xtab)
})
})
cat("Started with options:\r\n")
cat("* SMOTE : ", input$chkSVMWithSMOTE,"\r\n")
confusionMatrix(xtab)
runApp()
runApp()
data[,yVar] <- as.factor(data[,yVar])
runApp()
data[,yVar] <- as.factor(data[,yVar])
sample = sample.split(data[,yVar], SplitRatio = .75)
data_train = subset(copy(data), sample == TRUE)
data_test = subset(copy(data), sample == FALSE)
top100_cols <- findTop100Features(data_train,3);
v1 <- c()
for(i in 1: length(data))
{
if(class(data[,i]) == 'integer' || class(data[,i]) == 'numeric')
{
v1 <-  c(v1,names(data[i]))
#print(v1)
}
}
v1 <- c(v1,yVar)
data <- subset(data,select=v1);
data_old <- data
print(yVar)
table(data[,yVar]) #Statistics before
print(table(data[,yVar]))
names(data_old)[names(data_old) == yVar] <- "yVar"
data_balanced <- SMOTE(form = yVar ~., data = data_old, perc.over = 500,perc.under = 120)
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(data_old)[names(data_old) == "yVar"] <- yVar
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
library(entropy)
H.x <- entropy(table(data[,yVar]))
v1 <- c()
for(i in 1: length(data))
{
if(class(data[,i]) == 'integer' || class(data[,i]) == 'numeric')
{
v1 <-  c(v1,names(data[i]))
}
}
data_value <- subset(data,select=v1);
mi <- apply(data_value, 2, function(col) { H.x + entropy(table(col)) - entropy(table(data[,yVar], col))})
sort_vect <- sort(mi, decreasing=TRUE)
sort_vect <- as.data.frame(sort_vect)
d3 <- data.frame(row.names(sort_vect),sort_vect$sort_vect)
names(d3)<-c("Variable","importance")
d2 <- data.frame(names(sort_vect), t(sort_vect))
importance.v <- data.frame(Variable=d3$Variable,Importance=d3$importance);
importance.v[1:3,1]
importance.v$Variable
class(importance.v[1:3,1])
as.vector(importance.v[1:3,1])
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
v <- as.vector(importance.v[1:3,1])
data_balanced <- subset(data_balanced,select=c(v,yVar));
runApp()
runApp()
runApp()
runApp()
library(entropy)
H.x <- entropy(table(data[,yVar]))
v1 <- c()
for(i in 1: length(data))
{
if(class(data[,i]) == 'integer' || class(data[,i]) == 'numeric')
{
v1 <-  c(v1,names(data[i]))
}
}
data_value <- subset(data,select=v1);
mi <- apply(data_value, 2, function(col) { H.x + entropy(table(col)) - entropy(table(data[,yVar], col))})
sort_vect <- sort(mi, decreasing=TRUE)
sort_vect <- as.data.frame(sort_vect)
d3 <- data.frame(row.names(sort_vect),sort_vect$sort_vect)
names(d3)<-c("Variable","importance")
d2 <- data.frame(names(sort_vect), t(sort_vect))
importance.v <- data.frame(Variable=d3$Variable,Importance=d3$importance);
str(importance.v)
runApp()
runApp()
data[,yVar] <- as.factor(data[,yVar])
sample = sample.split(data[,yVar], SplitRatio = .75)
data_train = subset(copy(data), sample == TRUE)
data_test = subset(copy(data), sample == FALSE)
top100_cols <- findTop100Features(data_train,input$intNoCorrVars2);
top100_cols <- findTop100Features(data_train,10);
top100_cols <- as.vector(top100_cols[,1])
v1 <- c()
for(i in 1: length(data))
{
if(class(data[,i]) == 'integer' || class(data[,i]) == 'numeric')
{
v1 <-  c(v1,names(data[i]))
#print(v1)
}
}
v1 <- c(v1,yVar)
data <- subset(data,select=v1);
data_old <- data
print(yVar)
table(data[,yVar]) #Statistics before
print(table(data[,yVar]))
names(data_old)[names(data_old) == yVar] <- "yVar"
data_balanced <- SMOTE(form = yVar ~., data = data_old, perc.over = 500,perc.under = 120)
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(data_old)[names(data_old) == "yVar"] <- yVar
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
columns.to.keep<-names(which(colMeans(is.na(data_balanced)) < 0.5)) # this removes those columns with more than 50% NULLs
data_balanced<-subset(data_balanced,select = columns.to.keep) #the columns will stay which has less than 50% NAs
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
test1.nona <- data_balanced[ , colSums(is.na(data_balanced)) != 0]
if(ncol(test1.nona) > 0)
{
data_balanced <- rfImpute(yVar ~ ., data=data_balanced, iter=2, ntree=30)
}else{
data_balanced <- data_balanced;
}
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
nzv <- nearZeroVar(data_balanced)
if(length(nzv) > 0)
{
data_balanced <- data_balanced[,-nzv]
}
data_balanced[is.na(data_balanced)]<--9999
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
xData <- subset(data_balanced,select=-yVar);
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(xData)[names(xData) == "yVar"] <- yVar
yData <- data_balanced[,yVar];
fit.svm<-svm(x= xData,y= yData)
test_data <- data_test
test_result <- test_data[,yVar]; # Make sure we have the results seaprate - as some functions like it separate
names(test_data)[names(test_data) == yVar] <- "yVar"
test_data <- subset(test_data,select=-yVar);
names(test_data)[names(test_data) == "yVar"] <- yVar
test_data[is.na(test_data)]<--9999 #just some random number that never happened in the data
test_data<-subset(test_data,select = c(top100_cols));
test_data<-subset(test_data,select = setdiff(columns.to.keep,yVar)) #the columns will stay which has less than 50% NAs
if(length(nzv) > 0)
{
test_data <- test_data[,-nzv]
}
pred_data <- predict(object = fit.svm, newdata = test_data)
xtab <- table(pred_data, test_result)
data <- data
cat("Started with options:\r\n")
cat("* SMOTE : ", input$chkSVMWithSMOTE,"\r\n")
cat("* TOP100 Features : ",input$chkSVMWithTOPFeatures,"\r\n")
cat("* TEST/TRAINING SET : ",input$chkSVMWithTest,"\r\n")
cat("---------------------------------------------------------\r\n")
confusionMatrix(xtab)
runApp()
runApp()
class(data$Circuit)
class(data$Petitioner)
v1 <- c()
v1 <- c()
for(i in 1: length(data))
{
if(class(data[,i]) == 'integer' || class(data[,i]) == 'numeric')
{
v1 <-  c(v1,names(data[i]))
}
}
v1 <- c(v1,yVar)
data_old <- subset(data,select=v1);
data_value <- copy(data_old[,1:ncol(data_old)]);
stat.desc(data_value)
v1 <- c()
for(i in 1: length(data))
{
if(class(data[,i]) == 'integer' || class(data[,i]) == 'numeric')
{
v1 <-  c(v1,names(data[i]))
}
}
data_old <- subset(data,select=v1);
data_value <- copy(data_old[,1:ncol(data_old)]);
stat.desc(data_value)
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
data[,yVar] <- as.factor(data[,yVar])
sample = sample.split(data[,yVar], SplitRatio = .75)
data_train = subset(copy(data), sample == TRUE)
data_test = subset(copy(data), sample == FALSE)
top100_cols <- findTop100Features(data_train,input$intNoCorrVars2);
top100_cols <- findTop100Features(data_train,3);
top100_cols <- as.vector(top100_cols[,1])
v1 <- c()
for(i in 1: length(data))
{
if(class(data[,i]) == 'integer' || class(data[,i]) == 'numeric')
{
v1 <-  c(v1,names(data[i]))
#print(v1)
}
}
v1 <- c(v1,yVar)
data <- subset(data,select=v1);
data_old <- data
print(yVar)
table(data[,yVar]) #Statistics before
print(table(data[,yVar]))
names(data_old)[names(data_old) == yVar] <- "yVar"
data_balanced <- SMOTE(form = yVar ~., data = data_old, perc.over = 500,perc.under = 120)
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
columns.to.keep<-names(which(colMeans(is.na(data_balanced)) < 0.5)) # this removes those columns with more than 50% NULLs
data_balanced<-subset(data_balanced,select = columns.to.keep) #the columns will stay which has less than 50% NAs
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
test1.nona <- data_balanced[ , colSums(is.na(data_balanced)) != 0]
if(ncol(test1.nona) > 0)
{
data_balanced <- rfImpute(yVar ~ ., data=data_balanced, iter=2, ntree=30)
}else{
data_balanced <- data_balanced;
}
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
nzv <- nearZeroVar(data_balanced)
if(length(nzv) > 0)
{
data_balanced <- data_balanced[,-nzv]
}
data_balanced[is.na(data_balanced)]<--9999
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
xData <- subset(data_balanced,select=-yVar);
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(xData)[names(xData) == "yVar"] <- yVar
yData <- data_balanced[,yVar];
fit.svm<-svm(x= xData,y= yData)
test_data <- data_test
test_result <- test_data[,yVar]; # Make sure we have the results seaprate - as some functions like it separate
names(test_data)[names(test_data) == yVar] <- "yVar"
test_data <- subset(test_data,select=-yVar);
names(test_data)[names(test_data) == "yVar"] <- yVar
test_data[is.na(test_data)]<--9999 #just some random number that never happened in the data
{
test_data<-subset(test_data,select = c(top100_cols));
}
test_data<-subset(test_data,select = c(top100_cols));
test_data<-subset(test_data,select = setdiff(columns.to.keep,yVar)) #the columns will stay which has less than 50% NAs
if(length(nzv) > 0)
{
test_data <- test_data[,-nzv]
}
pred_data <- predict(object = fit.svm, newdata = test_data)
xtab <- table(pred_data, test_result)
data <- data
runApp()
data[,yVar] <- as.factor(data[,yVar])
sample = sample.split(data[,yVar], SplitRatio = .75)
data_train = subset(copy(data), sample == TRUE)
data_test = subset(copy(data), sample == FALSE)
top100_cols <- findTop100Features(data_train,input$intNoCorrVars2);
top100_cols <- as.vector(top100_cols[,1])
top100_cols <- findTop100Features(data_train,3);
top100_cols <- as.vector(top100_cols[,1])
v1 <- c()
data_old <- data
for(i in 1: length(data_old))
{
if(class(data_old[,i]) == 'integer' || class(data_old[,i]) == 'numeric')
{
v1 <-  c(v1,names(data_old[i]))
#print(v1)
}
}
v1 <- c(v1,yVar)
data_old <- subset(data_old,select=v1);
print(yVar)
table(data[,yVar]) #Statistics before
print(table(data[,yVar]))
names(data_old)[names(data_old) == yVar] <- "yVar"
data_balanced <- SMOTE(form = yVar ~., data = data_old, perc.over = 500,perc.under = 120)
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(data_old)[names(data_old) == "yVar"] <- yVar
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
columns.to.keep<-names(which(colMeans(is.na(data_balanced)) < 0.5)) # this removes those columns with more than 50% NULLs
data_balanced<-subset(data_balanced,select = columns.to.keep) #the columns will stay which has less than 50% NAs
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
test1.nona <- data_balanced[ , colSums(is.na(data_balanced)) != 0]
if(ncol(test1.nona) > 0)
{
data_balanced <- rfImpute(yVar ~ ., data=data_balanced, iter=2, ntree=30)
}else{
data_balanced <- data_balanced;
}
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
nzv <- nearZeroVar(data_balanced)
if(length(nzv) > 0)
{
data_balanced <- data_balanced[,-nzv]
}
data_balanced[is.na(data_balanced)]<--9999
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
xData <- subset(data_balanced,select=-yVar);
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(xData)[names(xData) == "yVar"] <- yVar
yData <- data_balanced[,yVar];
fit.svm<-svm(x= xData,y= yData)
test_data <- data_test
test_result <- test_data[,yVar]; # Make sure we have the results seaprate - as some functions like it separate
names(test_data)[names(test_data) == yVar] <- "yVar"
test_data <- subset(test_data,select=-yVar);
names(test_data)[names(test_data) == "yVar"] <- yVar
test_data[is.na(test_data)]<--9999 #just some random number that never happened in the data
if (input$chkSVMWithTOPFeatures == TRUE)
test_data<-subset(test_data,select = c(top100_cols));
test_data<-subset(test_data,select = c(top100_cols));
test_data<-subset(test_data,select = setdiff(columns.to.keep,yVar)) #the columns will stay which has less than 50% NAs
if(length(nzv) > 0)
{
test_data <- test_data[,-nzv]
}
pred_data <- predict(object = fit.svm, newdata = test_data)
xtab <- table(pred_data, test_result)
data <- data
output$Out_svm = renderPrint({
isolate({
cat("Started with options:\r\n")
cat("* SMOTE : ", input$chkSVMWithSMOTE,"\r\n")
cat("* TOP100 Features : ",input$chkSVMWithTOPFeatures,"\r\n")
cat("* TEST/TRAINING SET : ",input$chkSVMWithTest,"\r\n")
cat("---------------------------------------------------------\r\n")
confusionMatrix(xtab)
})
})
runApp()
shiny::runApp()
install.packages("shinyBS")
runApp()
data[,yVar] <- as.factor(data[,yVar])
sample = sample.split(data[,yVar], SplitRatio = .75)
data_train = subset(copy(data), sample == TRUE)
data_test = subset(copy(data), sample == FALSE)
data_train <- copy(data);
data_test <- copy(data);
top100_cols <- findTop100Features(data_train,input$intNoCorrVars2);
top100_cols <- as.vector(top100_cols[,1])
v1 <- c()
data_old <- data
for(i in 1: length(data_old))
{
if(class(data_old[,i]) == 'integer' || class(data_old[,i]) == 'numeric')
{
v1 <-  c(v1,names(data_old[i]))
#print(v1)
}
}
v1 <- c(v1,yVar)
data_old <- subset(data_old,select=v1);
print(yVar)
table(data[,yVar]) #Statistics before
print(table(data[,yVar]))
names(data_old)[names(data_old) == yVar] <- "yVar"
data_balanced <- SMOTE(form = yVar ~., data = data_old, perc.over = 500,perc.under = 120)
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(data_old)[names(data_old) == "yVar"] <- yVar
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
columns.to.keep<-names(which(colMeans(is.na(data_balanced)) < 0.5)) # this removes those columns with more than 50% NULLs
data_balanced<-subset(data_balanced,select = columns.to.keep) #the columns will stay which has less than 50% NAs
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
test1.nona <- data_balanced[ , colSums(is.na(data_balanced)) != 0]
if(ncol(test1.nona) > 0)
{
data_balanced <- rfImpute(yVar ~ ., data=data_balanced, iter=2, ntree=30)
}else{
data_balanced <- data_balanced;
}
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
nzv <- nearZeroVar(data_balanced)
if(length(nzv) > 0)
{
data_balanced <- data_balanced[,-nzv]
}
data_balanced[is.na(data_balanced)]<--9999
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
xData <- subset(data_balanced,select=-yVar);
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(xData)[names(xData) == "yVar"] <- yVar
yData <- data_balanced[,yVar];
fit.svm<-svm(x= xData,y= yData)
View(xData)
lapply(xData,class)
lapply(yData,class)
runApp()
runApp()
runApp()
data[,yVar] <- as.factor(data[,yVar])
sample = sample.split(data[,yVar], SplitRatio = .75)
data_train = subset(copy(data), sample == TRUE)
data_test = subset(copy(data), sample == FALSE)
top100_cols <- findTop100Features(data_train,input$intNoCorrVars2);
top100_cols <- as.vector(top100_cols[,1])
top100_cols <- findTop100Features(data_train,3);
top100_cols <- as.vector(top100_cols[,1])
v1 <- c()
data_old <- data
for(i in 1: length(data_old))
{
if(class(data_old[,i]) == 'integer' || class(data_old[,i]) == 'numeric')
{
v1 <-  c(v1,names(data_old[i]))
#print(v1)
}
}
v1 <- c(v1,yVar)
data_old <- subset(data_old,select=v1);
print(yVar)
table(data[,yVar]) #Statistics before
print(table(data[,yVar]))
names(data_old)[names(data_old) == yVar] <- "yVar"
data_balanced <- SMOTE(form = yVar ~., data = data_old, perc.over = 500,perc.under = 120)
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(data_old)[names(data_old) == "yVar"] <- yVar
data_balanced <- subset(data_balanced,select=c(top100_cols,yVar));
columns.to.keep<-names(which(colMeans(is.na(data_balanced)) < 0.5)) # this removes those columns with more than 50% NULLs
data_balanced<-subset(data_balanced,select = columns.to.keep) #the columns will stay which has less than 50% NAs
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
test1.nona <- data_balanced[ , colSums(is.na(data_balanced)) != 0]
if(ncol(test1.nona) > 0)
{
data_balanced <- rfImpute(yVar ~ ., data=data_balanced, iter=2, ntree=30)
}else{
data_balanced <- data_balanced;
}
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
nzv <- nearZeroVar(data_balanced)
if(length(nzv) > 0)
{
data_balanced <- data_balanced[,-nzv]
}
data_balanced[is.na(data_balanced)]<--9999
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
xData <- subset(data_balanced,select=-yVar);
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(xData)[names(xData) == "yVar"] <- yVar
yData <- data_balanced[,yVar];
XData <- xData[,-2]
View(xData)
xData <- subset(data_balanced,select=-yVar);
data_balanced[is.na(data_balanced)]<--9999
names(data_balanced)[names(data_balanced) == yVar] <- "yVar"
xData <- subset(data_balanced,select=-yVar);
names(data_balanced)[names(data_balanced) == "yVar"] <- yVar
names(xData)[names(xData) == "yVar"] <- yVar
yData <- data_balanced[,yVar];
XData <- xData[,-1]
fit.svm<-svm(x = xData,y= yData)
View(xData)
View(xData)
xData <- subset(xData,select=-Unconst);
View(xData)
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
installed.packages("rattle")
installed.packages(rattle)
install.packages('rattle'')
install.packages('rattle')
install.packages('rattle')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
